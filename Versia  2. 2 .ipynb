{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":9166746,"sourceType":"datasetVersion","datasetId":5538891}],"dockerImageVersionId":30761,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/andrey36912/notebooke9cda60ba3?scriptVersionId=195207356\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-09-03T19:09:43.548594Z","iopub.execute_input":"2024-09-03T19:09:43.549122Z","iopub.status.idle":"2024-09-03T19:09:43.558811Z","shell.execute_reply.started":"2024-09-03T19:09:43.549076Z","shell.execute_reply":"2024-09-03T19:09:43.557576Z"},"trusted":true},"execution_count":38,"outputs":[{"name":"stdout","text":"/kaggle/input/exam-1/data.csv\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Побудова моделі для задач класифікації та регресії\n\nпри необхідності згенерувати нові ознаки\nсамостійно обрати модель для тренування або обрати за допомогою optuna\nрозділити дані з викидами на тренувальні та тестові, можливо валідаційні якщо потрібно\nвибрати метрики для оцінки якості моделі, вибір пояснити\nпобудувати препроцесор\nнатренувати модель\nвивести метрики для тестових даних, дати оцінку якості моделі\nзберегти натреновану модель","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split, cross_val_score\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import OneHotEncoder, StandardScaler\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.linear_model import LinearRegression, Ridge\nfrom sklearn.ensemble import RandomForestRegressor\nimport optuna\nfrom sklearn.compose import make_column_selector","metadata":{"execution":{"iopub.status.busy":"2024-09-03T19:09:43.561105Z","iopub.execute_input":"2024-09-03T19:09:43.561563Z","iopub.status.idle":"2024-09-03T19:09:43.574005Z","shell.execute_reply.started":"2024-09-03T19:09:43.561514Z","shell.execute_reply":"2024-09-03T19:09:43.572478Z"},"trusted":true},"execution_count":39,"outputs":[]},{"cell_type":"code","source":"df = pd.read_csv('/kaggle/input/exam-1/data.csv')","metadata":{"execution":{"iopub.status.busy":"2024-09-03T19:09:43.575705Z","iopub.execute_input":"2024-09-03T19:09:43.576138Z","iopub.status.idle":"2024-09-03T19:09:44.425712Z","shell.execute_reply.started":"2024-09-03T19:09:43.576096Z","shell.execute_reply":"2024-09-03T19:09:44.424431Z"},"trusted":true},"execution_count":40,"outputs":[]},{"cell_type":"code","source":"def convert_stay_to_days(stay):\n    stay_ranges = {\n        '0-10': 5,\n        '11-20': 15,\n        '21-30': 25,\n        '31-40': 35,\n        '41-50': 45,\n        '51-60': 55,\n        '61-70': 65,\n        '71-80': 75,\n        '81-90': 85,\n        '91-100': 95,\n        'More than 100 Days': 110\n    }\n    return stay_ranges.get(stay, np.nan)\n\ndf['Stay_Days'] = df['Stay'].apply(convert_stay_to_days)","metadata":{"execution":{"iopub.status.busy":"2024-09-03T19:09:44.427218Z","iopub.execute_input":"2024-09-03T19:09:44.427607Z","iopub.status.idle":"2024-09-03T19:09:44.905993Z","shell.execute_reply.started":"2024-09-03T19:09:44.427567Z","shell.execute_reply":"2024-09-03T19:09:44.904515Z"},"trusted":true},"execution_count":41,"outputs":[]},{"cell_type":"code","source":"df = df.drop(columns=['Stay', 'Untitled'], errors='ignore')","metadata":{"execution":{"iopub.status.busy":"2024-09-03T19:09:44.909199Z","iopub.execute_input":"2024-09-03T19:09:44.909961Z","iopub.status.idle":"2024-09-03T19:09:44.954598Z","shell.execute_reply.started":"2024-09-03T19:09:44.909913Z","shell.execute_reply":"2024-09-03T19:09:44.95334Z"},"trusted":true},"execution_count":42,"outputs":[]},{"cell_type":"code","source":"y_regression = df['Stay_Days'] \nX = df.drop(columns=['Stay_Days'], errors='ignore')","metadata":{"execution":{"iopub.status.busy":"2024-09-03T19:09:44.95621Z","iopub.execute_input":"2024-09-03T19:09:44.956875Z","iopub.status.idle":"2024-09-03T19:09:45.000806Z","shell.execute_reply.started":"2024-09-03T19:09:44.956807Z","shell.execute_reply":"2024-09-03T19:09:44.99942Z"},"trusted":true},"execution_count":43,"outputs":[]},{"cell_type":"code","source":"numerical_features = make_column_selector(dtype_include=np.number)\ncategorical_features = make_column_selector(dtype_include=object)\nnumerical_features = numerical_features(X)\ncategorical_features = categorical_features(X)","metadata":{"execution":{"iopub.status.busy":"2024-09-03T19:09:45.002944Z","iopub.execute_input":"2024-09-03T19:09:45.003394Z","iopub.status.idle":"2024-09-03T19:09:45.009889Z","shell.execute_reply.started":"2024-09-03T19:09:45.003316Z","shell.execute_reply":"2024-09-03T19:09:45.00859Z"},"trusted":true},"execution_count":44,"outputs":[]},{"cell_type":"code","source":"for feature in numerical_features:\n    X[feature] = X[feature].fillna(X[feature].median())\nfor feature in categorical_features:\n    X[feature] = X[feature].fillna(X[feature].mode()[0])","metadata":{"execution":{"iopub.status.busy":"2024-09-03T19:09:45.01147Z","iopub.execute_input":"2024-09-03T19:09:45.011889Z","iopub.status.idle":"2024-09-03T19:09:45.851371Z","shell.execute_reply.started":"2024-09-03T19:09:45.011849Z","shell.execute_reply":"2024-09-03T19:09:45.849967Z"},"trusted":true},"execution_count":45,"outputs":[]},{"cell_type":"code","source":"Q1 = X[numerical_features].quantile(0.25)\nQ3 = X[numerical_features].quantile(0.75)\nIQR = Q3 - Q1\nlower_bound = Q1 - 1.5 * IQR\nupper_bound = Q3 + 1.5 * IQR\n\nX_cleaned = X[~((X[numerical_features] < lower_bound) | (X[numerical_features] > upper_bound)).any(axis=1)]\ny_cleaned = y_regression[X_cleaned.index]  # Приведение y к размеру очищенных данных\nprint(\"Розмір даних до видалення викидів:\", X.shape)\nprint(\"Розмір даних після видалення викидів:\", X_cleaned.shape)","metadata":{"execution":{"iopub.status.busy":"2024-09-03T19:09:45.852795Z","iopub.execute_input":"2024-09-03T19:09:45.853156Z","iopub.status.idle":"2024-09-03T19:09:46.090177Z","shell.execute_reply.started":"2024-09-03T19:09:45.85312Z","shell.execute_reply":"2024-09-03T19:09:46.089018Z"},"trusted":true},"execution_count":46,"outputs":[{"name":"stdout","text":"Розмір даних до видалення викидів: (318438, 20)\nРозмір даних після видалення викидів: (270882, 20)\n","output_type":"stream"}]},{"cell_type":"code","source":"numeric_transformer = Pipeline(steps=[\n    ('imputer', SimpleImputer(strategy='median')),\n    ('scaler', StandardScaler())\n])\n\ncategorical_transformer = Pipeline(steps=[\n    ('imputer', SimpleImputer(strategy='most_frequent')),\n    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n])","metadata":{"execution":{"iopub.status.busy":"2024-09-03T19:09:46.091457Z","iopub.execute_input":"2024-09-03T19:09:46.09182Z","iopub.status.idle":"2024-09-03T19:09:46.098498Z","shell.execute_reply.started":"2024-09-03T19:09:46.091781Z","shell.execute_reply":"2024-09-03T19:09:46.097288Z"},"trusted":true},"execution_count":47,"outputs":[]},{"cell_type":"code","source":"preprocessor = ColumnTransformer(\n    transformers=[\n        ('num', numeric_transformer, numerical_features),  \n        ('cat', categorical_transformer, categorical_features)  \n    ]\n)","metadata":{"execution":{"iopub.status.busy":"2024-09-03T19:09:46.099702Z","iopub.execute_input":"2024-09-03T19:09:46.100055Z","iopub.status.idle":"2024-09-03T19:09:46.109419Z","shell.execute_reply.started":"2024-09-03T19:09:46.100017Z","shell.execute_reply":"2024-09-03T19:09:46.108116Z"},"trusted":true},"execution_count":48,"outputs":[]},{"cell_type":"code","source":"X_train_reg, X_test_reg, y_train_reg, y_test_reg = train_test_split(X_cleaned, y_cleaned, test_size=0.2, random_state=42)","metadata":{"execution":{"iopub.status.busy":"2024-09-03T19:09:46.110714Z","iopub.execute_input":"2024-09-03T19:09:46.11118Z","iopub.status.idle":"2024-09-03T19:09:46.301766Z","shell.execute_reply.started":"2024-09-03T19:09:46.111128Z","shell.execute_reply":"2024-09-03T19:09:46.300683Z"},"trusted":true},"execution_count":49,"outputs":[]},{"cell_type":"code","source":"def objective_regression(trial):\n    regressor_name = trial.suggest_categorical('regressor', ['LinearRegression', 'Ridge', 'RandomForest'])\n    \n    if regressor_name == 'LinearRegression':\n        regressor = LinearRegression()\n    elif regressor_name == 'Ridge':\n        alpha = trial.suggest_float('alpha', 1e-5, 10.0)\n        regressor = Ridge(alpha=alpha)\n    elif regressor_name == 'RandomForest':\n        max_depth = trial.suggest_int('max_depth', 2, 32)\n        n_estimators = trial.suggest_int('n_estimators', 10, 300)\n        regressor = RandomForestRegressor(max_depth=max_depth, n_estimators=n_estimators)\n    \n    pipeline = Pipeline(steps=[('preprocessor', preprocessor),\n                               ('regressor', regressor)])\n    \n    scores = cross_val_score(pipeline, X_train_reg, y_train_reg, cv=3, scoring='neg_mean_squared_error')\n    mse = -scores.mean()\n    \n    return mse","metadata":{"execution":{"iopub.status.busy":"2024-09-03T19:09:46.303231Z","iopub.execute_input":"2024-09-03T19:09:46.303662Z","iopub.status.idle":"2024-09-03T19:09:46.313049Z","shell.execute_reply.started":"2024-09-03T19:09:46.303621Z","shell.execute_reply":"2024-09-03T19:09:46.311773Z"},"trusted":true},"execution_count":50,"outputs":[]},{"cell_type":"code","source":"study_regression = optuna.create_study(direction='minimize')\nstudy_regression.optimize(objective_regression, n_trials=50)","metadata":{"execution":{"iopub.status.busy":"2024-09-03T19:09:46.316871Z","iopub.execute_input":"2024-09-03T19:09:46.317264Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stderr","text":"[I 2024-09-03 19:09:46,327] A new study created in memory with name: no-name-1ad1cb9f-5aaa-495b-aca5-874f6a8c1d1d\n[I 2024-09-03 19:09:50,863] Trial 0 finished with value: 270.361038182287 and parameters: {'regressor': 'Ridge', 'alpha': 4.217166112598749}. Best is trial 0 with value: 270.361038182287.\n[I 2024-09-03 19:09:55,439] Trial 1 finished with value: 270.360789788639 and parameters: {'regressor': 'Ridge', 'alpha': 7.209622806540318}. Best is trial 1 with value: 270.360789788639.\n[I 2024-09-03 19:18:43,393] Trial 2 finished with value: 236.58478583543499 and parameters: {'regressor': 'RandomForest', 'max_depth': 27, 'n_estimators': 87}. Best is trial 2 with value: 236.58478583543499.\n[I 2024-09-03 19:18:48,760] Trial 3 finished with value: 270.3627746236077 and parameters: {'regressor': 'LinearRegression'}. Best is trial 2 with value: 236.58478583543499.\n[I 2024-09-03 19:27:00,560] Trial 4 finished with value: 236.31792702076356 and parameters: {'regressor': 'RandomForest', 'max_depth': 26, 'n_estimators': 81}. Best is trial 4 with value: 236.31792702076356.\n[I 2024-09-03 19:27:06,763] Trial 5 finished with value: 270.3627746236077 and parameters: {'regressor': 'LinearRegression'}. Best is trial 4 with value: 236.31792702076356.\n[I 2024-09-03 19:38:40,853] Trial 6 finished with value: 236.34527197161705 and parameters: {'regressor': 'RandomForest', 'max_depth': 31, 'n_estimators': 111}. Best is trial 4 with value: 236.31792702076356.\n[I 2024-09-03 19:38:45,376] Trial 7 finished with value: 270.3607699694701 and parameters: {'regressor': 'Ridge', 'alpha': 8.224832581612576}. Best is trial 4 with value: 236.31792702076356.\n[I 2024-09-03 19:54:19,119] Trial 8 finished with value: 239.63646472946797 and parameters: {'regressor': 'RandomForest', 'max_depth': 10, 'n_estimators': 290}. Best is trial 4 with value: 236.31792702076356.\n[I 2024-09-03 19:54:24,439] Trial 9 finished with value: 270.3627746236077 and parameters: {'regressor': 'LinearRegression'}. Best is trial 4 with value: 236.31792702076356.\n","output_type":"stream"}]},{"cell_type":"code","source":"best_params_reg = study_regression.best_trial.params\nif best_params_reg['regressor'] == 'LinearRegression':\n    best_model_reg = LinearRegression()\nelif best_params_reg['regressor'] == 'Ridge':\n    best_model_reg = Ridge(alpha=best_params_reg['alpha'])\nelse:\n    best_model_reg = RandomForestRegressor(max_depth=best_params_reg['max_depth'], n_estimators=best_params_reg['n_estimators'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pipeline_regression = Pipeline(steps=[('preprocessor', preprocessor), ('regressor', best_model_reg)])\npipeline_regression.fit(X_train_reg, y_train_reg)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_pred_reg = pipeline_regression.predict(X_test_reg)\nmse_reg = mean_squared_error(y_test_reg, y_pred_reg)\nr2_reg = r2_score(y_test_reg, y_pred_reg)\n\nprint(f'Mean Squared Error (Регресія): {mse_reg}')\nprint(f'R^2 Score (Регресія): {r2_reg}')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"joblib.dump(pipeline_regression, 'best_regression_model.pkl')\n\nX = df_cleaned.drop(columns=['Stay_Days', 'Stay'])\ny_classification = df_cleaned['Stay']\n\nX_train_cls, X_test_cls, y_train_cls, y_test_cls = train_test_split(X, y_classification, test_size=0.2, random_state=42)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def objective_classification(trial):\n    classifier_name = trial.suggest_categorical('classifier', ['LogisticRegression', 'RandomForest'])\n    \n    if classifier_name == 'LogisticRegression':\n        C = trial.suggest_float('C', 1e-5, 10.0)\n        classifier = LogisticRegression(C=C, max_iter=1000)\n    elif classifier_name == 'RandomForest':\n        max_depth = trial.suggest_int('max_depth', 2, 32)\n        n_estimators = trial.suggest_int('n_estimators', 10, 300)\n        classifier = RandomForestClassifier(max_depth=max_depth, n_estimators=n_estimators)\n    \n    pipeline = Pipeline(steps=[('preprocessor', preprocessor),\n                               ('classifier', classifier)])\n    \n    scores = cross_val_score(pipeline, X_train_cls, y_train_cls, cv=3, scoring='accuracy')\n    accuracy = scores.mean()\n    \n    return accuracy\n\nstudy_classification = optuna.create_study(direction='maximize')\nstudy_classification.optimize(objective_classification, n_trials=50)\n\nbest_params_cls = study_classification.best_trial.params\nif best_params_cls['classifier'] == 'LogisticRegression':\n    best_model_cls = LogisticRegression(C=best_params_cls['C'], max_iter=1000)\nelse:\n    best_model_cls = RandomForestClassifier(max_depth=best_params_cls['max_depth'], n_estimators=best_params_cls['n_estimators'])\n\npipeline_classification = Pipeline(steps=[('preprocessor', preprocessor), ('classifier', best_model_cls)])\npipeline_classification.fit(X_train_cls, y_train_cls)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_pred_cls = pipeline_classification.predict(X_test_cls)\naccuracy_cls = accuracy_score(y_test_cls, y_pred_cls)\nf1_cls = f1_score(y_test_cls, y_pred_cls, average='weighted')\n\nprint(f'Accuracy (Класифікація): {accuracy_cls}')\nprint(f'F1 Score (Класифікація): {f1_cls}')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"joblib.dump(pipeline_classification, 'best_classification_model.pkl')","metadata":{},"execution_count":null,"outputs":[]}]}